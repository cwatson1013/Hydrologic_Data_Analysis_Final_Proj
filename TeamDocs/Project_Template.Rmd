---
output: 
  pdf_document:
    keep_tex: yes
    fig_caption: yes
    number_sections: yes
geometry: margin=2.54cm
title: Examining the Hydrologic Properties of the Missouri River Basin
subtitle: https://github.com/cwatson1013/Hydrologic_Data_Analysis_Final_Proj
author: Rachel Bash, Keqi He, Caroline Watson, and Haoyu Zhang
abstract: "The Missouri River provides critical water resources that drives the region's agriculture, industry, and ecosystems. This is a region that experiences surface water variability, characterized by damaging floods and severe droughts, greatly impacting the agricultural production of the area. This project highlights the changes in streamflow and water quality over time, and identifies key characteristics of the river....Twenty six sites across the lower Missouri River Basin were examined in order to get a fuller picture of the Missouri River and its tributaries over time."
fontsize: 12pt
mainfont: Times New Roman
editor_options: 
  chunk_output_type: console
---

<Information in these brackets are used for annotating the RMarkdown file. They will not appear in the final version of the PDF document>

\newpage
\tableofcontents 
\newpage
\listoftables 
\newpage
\listoffigures 
\newpage

<Setup the global options for the R chunks in your document>

<Note: set up autoreferencing for figures and tables in your document>

```{r setup, include=FALSE, warning=FALSE, message=FALSE}
# Set your working directory

getwd()

# Load your packages
library(tidyverse)
library(kableExtra)
library(cowplot)
library(dataRetrieval)

# Set your ggplot theme
theme_set(theme_classic())
```


```{r, echo=FALSE}
# Loading in data

#reading in daily value data
bestsites.DNP <- read.csv("../Data/Raw/bestsites.DNP.csv")

#reading in water quality data
bestsites.wq <- read.csv("../Data/Raw/bestsites.WQ.csv")

#reading in high freq data
highfreqsiteinfo <- read.csv("../Data/Raw/highfreqsiteinfo.csv")


```


# Research Question and Rationale

<Paragraph detailing the rationale for your analysis. What is the significant application and/or interest in this topic? Connect to environmental topic(s)/challenge(s).>

<Paragraph detailing your research question(s) and goals. What do you want to find out? Include a sentence (or a few) on the dataset you are using to answer this question - just enough to give your reader an idea of where you are going with the analysis.>

\newpage

# Dataset Information

<Information on how the dataset for this analysis were collected, the data contained in the dataset, and any important pieces of information that are relevant to your analyses. This section should contain much of same information as the README file for the dataset but formatted in a way that is more narrative.>

The data we are analyzing comes from the United States Geological Survey (USGS) database called the National Water Information System interface, or NWIS. We pulled data from the interface using the R package `dataRetrieval`. Because we are interested in the lower Missouri River basin, we pulled sites from each HUC4 subbasin from 1020 to 1030 (see Figure below). We chose these subbasins because they had a variety of tributaries that all flowed into the Missouri River, and we wanted a variety of river sizes and lengths. We filtered these subbasin queries to only show us sites that had discharge, nitrogen, and phosphorus data. Once we found the sites with all of this data, we chose 2 sites from each HUC sub basin as our 22 "best sites". Our best sites had the overall best time period range for all of our "must have" variables.

Only seven sites within our HUC subbasin boundary contained any high frequency discharge and nitrogen data. Therefore, we also looked at these 7 sites in order to do analyses and answer our research question about flooding.

After doing initial data wrangling and analysis on our 22 "best sites", we decided to pare it down further and only do subsequent analyses on **10** sites. While we initially wanted to look at many sites that were varied in size and location, we determined that it was too many to look at and draw relevant conclusions from. 

We have three main datasets:

- The daily values dataset with our 22 "best sites"

- The water quality dataset with our 22 "best sites", with only six sites that had total coliform data.

- The high freqency dataset with 7 sites that contain both high freqency discharge and high frequency nitrogen data.

<Add a table that summarizes your data structure. This table can be made in markdown text or inserted as a `kable` function in an R chunk. If the latter, do not include the code used to generate your table.>

<C will do data table for water quality and daily values, R will do for high freq>


```{r, echo=FALSE, eval=TRUE, tbls="Summary of Daily Value Data at 22 sites in the Missouri River Basin", results="asis"}

#structure of daily value dataframe
dailyvalue.summary <- summary(bestsites.DNP)

#summary of data structure
kable(dailyvalue.summary, 
      caption = "Summary of Daily Value Data at 22 sites in the
      Missouri River Basin") %>% 
  kable_styling(latex_options = c("hold_position", "striped", 
                                  "scale_down")) %>% 
  kableExtra::landscape()

```

```{r, echo=FALSE, eval=TRUE, tbls="Summary of Water Quality Data at 22 sites in the Missouri River Basin", results="asis"}

#structure of water quality dataframe
waterquality.summary <- summary(bestsites.wq)

#summary of data structure
kable(waterquality.summary, 
      caption = "Summary of Water Quality Data in the
      Missouri River Basin") %>% 
  kable_styling(latex_options = c("hold_position", "striped", 
                                  "scale_down")) %>% 
  kableExtra::landscape() 

```

\newpage

# Exploratory Data Analysis and Wrangling

<Include R chunks for 5+ lines of summary code (display code and output), 3+ exploratory graphs (display graphs only), and any wrangling you do to your dataset(s).> 

<Include text sections to accompany these R chunks to explain the reasoning behind your workflow, and the rationale for your approach.>

```{r}
#high frequency data wrangling
highfreqsite2019 <- highfreqsiteinfo %>%
  filter(end_date > "2019-03-31"); head(highfreqsite2019)

highfreqsites.DN <- readNWISuv(site = c("06808500", "06817000", "06892350", "06934500"), 
                               parameterCd = c("00060", "99133"), 
                               # Discharge in cfs & Nitrate in mg/l NO3-N
                               startDate = "2019-01-01",
                               endDate = "2019-11-01") %>%
                               renameNWISColumns() %>%
                               rename(Nitrate_mgl = 6)

#individual sites
Hermann <- highfreqsites.DN %>%
           filter(site_no=="06934500")
Desoto <- highfreqsites.DN %>%
          filter(site_no=="06892350")
Clarinda <- highfreqsites.DN %>%
            filter(site_no=="06817000")
Randolph <- highfreqsites.DN %>%
            filter(site_no=="06808500")
```


### High Frequency Nitrogen and Discharge

There were 7 sites in our region of interest that had high freq N data, and only 4 sites had high freq N data during the floods of 2019. The sites looked at in depth are:

    - West Nishnabotna River in Randolph, IA
    - Nodaway River at Clarinda, IA
    - Kansas River in Desoto, KS
    - Missouri River at Hermann, MO

The Missouri River is the biggest river, with an average of 214693 cfs discharge rate during the year 2019, and the Nodaway River is the smallest river, with an average of 1185 cfs discharge rate for 2019.

In March of 2019, a [bomb cyclone](https://www.kansascity.com/news/state/missouri/article228237519.html) hit the midwest. Our initial research question, what effect did the March 2019 storm have on water quality, attempted to look into the behavior of nitrogen in the discharge of the rivers. Unfortunately, instantaneous Nitrogen values stopped recording during the peak of the storm events in March, so it was hard to create hysteresis plots that exhibited the type of storm and its effects on nitrogen concentration. 

Even though Nitrogen concentrations were not recorded in March, they were recorded in other times of the year. 2019 was a wet year and many large storm events occurred. 

```{r randolphs, echo=FALSE}
#Randolph storm
RandolphStorm <- Randolph %>%
  filter(dateTime > "2019-06-24" & dateTime < "2019-07-08") 


RandolphStorm.plot <- ggplot(RandolphStorm, aes(x = Flow_Inst, y = Nitrate_mgl, color = dateTime)) +
  geom_point() +
  labs(x="Discharge (cfs)", y= "Nitrogen mg/l)", color="Date", 
       title="West Nishnabotna River in Randolph, IA")

#semi-clockwise motion, negative slope, so diluting
```

```{r desotos, fig.cap="\\label{fig:desotos} Hysteresis plots", echo=FALSE}
#desoto storm

DesotoStorm <- Desoto %>%
  filter(dateTime > "2019-02-22" & dateTime < "2019-02-28") 


DesotoStorm.plot <- ggplot(DesotoStorm, aes(x = Flow_Inst, y = Nitrate_mgl, color = dateTime)) +
  geom_point() +
  labs(x="Discharge (cfs)", y= "Nitrogen mg/l)", color="Date", 
       title="Kansas River in Desoto, KS")


plot_grid(RandolphStorm.plot, DesotoStorm.plot, ncol=1)
#very loose counter clockwise motion - loose positive slope for flushing storm
```

The \autoref{fig:desotos} shows Hysteresis plots for two storm events in the Missouri River Basin. The storm event on the West Nishnabotna River exhibits an oddly-shaped plot that has a negative slope, indicating it is a diluting storm. The Kansas River experienced a storm in late February that has a counter-clockwise motion and a positive slope, indicating a flushing storm. These two plots illustrate that two rivers near each other can have very different behaviors.

\newpage

# Analysis
<Include R chunks for 3+ statistical tests (display code and output) and 3+ final visualization graphs (display graphs only).>

<Include text sections to accompany these R chunks to explain the reasoning behind your workflow, rationale for your approach, and the justification of meeting or failing to meet assumptions of tests.>


\newpage

# Summary and Conclusions
<Summarize your major findings from your analyses. What conclusions do you draw from your findings? Make sure to apply this to a broader application for the research question you have answered.>


## Example for autoreferencing

As seen by \autoref{fig:foo}, Absorbance values are not normally distributed. This is expected, as we are dealing with ecological data.

```{r foo, echo=FALSE, warning=FALSE, fig.cap="\\label{fig:foo}Absorbance frequency"}
ggplot(cars) +
  geom_point(aes(x=speed, y= dist)) +
  labs(x= "Frequency", y="Absorbance value")
```
